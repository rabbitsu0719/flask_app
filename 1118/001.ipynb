{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76cdb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.5.2 nltk-3.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8b6aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # 처음 한 번만 실행하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d9bfea",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ubuntu/nltk_data'\n    - '/home/ubuntu/flask_app/venv/nltk_data'\n    - '/home/ubuntu/flask_app/venv/share/nltk_data'\n    - '/home/ubuntu/flask_app/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m sentence = \u001b[33m\"\u001b[39m\u001b[33m안녕하세요. 오늘 날씨가 참 좋네요\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flask_app/venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ubuntu/nltk_data'\n    - '/home/ubuntu/flask_app/venv/nltk_data'\n    - '/home/ubuntu/flask_app/venv/share/nltk_data'\n    - '/home/ubuntu/flask_app/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentence = \"안녕하세요. 오늘 날씨가 참 좋네요\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53dc429",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'okt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m korean_sentence = \u001b[33m\"\u001b[39m\u001b[33m안녕하세요, 여러분. 만나서 반갑습니다. 이제 공부를 시작해 봅시다.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m korean_tokens = \u001b[43mokt\u001b[49m.morphs(korean_sentence)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(korean_tokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'okt' is not defined"
     ]
    }
   ],
   "source": [
    "korean_sentence = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 공부를 시작해 봅시다.\"\n",
    "korean_tokens = okt.morphs(korean_sentence)\n",
    "print(korean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c51eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52181eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_sentence = \"안녕하세요. 여러분. 만나서 반갑습니다. 이제 공부를 시작해 봅시다.\"\n",
    "korean_tokens = okt.morphs(korean_sentence)\n",
    "print(korean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be3a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/flask_app/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a13ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 준비 완료\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "print(\"OKT 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_sentence = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 공부를 시작해 봅시다.\"\n",
    "korean_tokens = okt.morphs(korean_sentence)\n",
    "print(korean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b17c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '공부', '를', '시작', '해', '봅시다', '.']\n"
     ]
    }
   ],
   "source": [
    "korean_sentence = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 공부를 시작해 봅시다.\"\n",
    "korean_tokens = okt.morphs(korean_sentence)\n",
    "print(korean_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd4f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df34bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40a6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb068a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"안녕하세요!\",\n",
    "    \"문 좀 열어줘.\",\n",
    "    \"현재 시각이 궁금해.\",\n",
    "    \"창문을 닫아.\",\n",
    "    \"오늘 날씨 어때?\",\n",
    "    \"도와줄 수 있어?\",\n",
    "    \"잘 지내?\",\n",
    "    \"동영상 틀어줘.\",\n",
    "    \"2 더하기 2는 뭐야?\",\n",
    "    \"영화 추천해줘.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4498d47e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'okt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m test_sentences:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     tokens = \u001b[43mokt\u001b[49m.morphs(s)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m문장: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> 토큰: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'okt' is not defined"
     ]
    }
   ],
   "source": [
    "for s in test_sentences:\n",
    "    tokens = okt.morphs(s)\n",
    "    print(f\"문장: {s} -> 토큰: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b95c25c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 준비 완료\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "print(\"OKT 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bfcef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 안녕하세요! -> 토큰: ['안녕하세요', '!']\n",
      "문장: 문 좀 열어줘. -> 토큰: ['문', '좀', '열어줘', '.']\n",
      "문장: 현재 시각이 궁금해. -> 토큰: ['현재', '시각', '이', '궁금해', '.']\n",
      "문장: 창문을 닫아. -> 토큰: ['창문', '을', '닫아', '.']\n",
      "문장: 오늘 날씨 어때? -> 토큰: ['오늘', '날씨', '어때', '?']\n",
      "문장: 도와줄 수 있어? -> 토큰: ['도와줄', '수', '있어', '?']\n",
      "문장: 잘 지내? -> 토큰: ['잘', '지내', '?']\n",
      "문장: 동영상 틀어줘. -> 토큰: ['동영상', '틀어줘', '.']\n",
      "문장: 2 더하기 2는 뭐야? -> 토큰: ['2', '더하기', '2', '는', '뭐', '야', '?']\n",
      "문장: 영화 추천해줘. -> 토큰: ['영화', '추천', '해줘', '.']\n"
     ]
    }
   ],
   "source": [
    "for s in test_sentences:\n",
    "    tokens = okt.morphs(s)\n",
    "    print(f\"문장: {s} -> 토큰: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5793df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(tokens):\n",
    "    greetings = [\"안녕\", \"안녕하세요\", \"하이\", \"헬로\"]\n",
    "    farewells = [\"잘 가\", \"안녕히 가세요\", \"바이\", \"굿바이\"]\n",
    "    time_queries = [\"시간\", \"몇 시\", \"현재 시각\"]\n",
    "    weather_queries = [\"날씨\", \"기상\", \"오늘 날씨\"]\n",
    "    help_requests = [\"도와줘\", \"도와줄 수 있어\", \"도움 필요해\"]\n",
    "    math_queries = [\"더하기\", \"빼기\", \"곱하기\", \"나누기\"]\n",
    "\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    if token_set.intersection(greetings):\n",
    "        return \"인사\"\n",
    "    elif token_set.intersection(farewells):\n",
    "        return \"작별 인사\"\n",
    "    elif token_set.intersection(time_queries):\n",
    "        return \"시간 문의\"\n",
    "    elif token_set.intersection(weather_queries):\n",
    "        return \"날씨 문의\"\n",
    "    elif token_set.intersection(help_requests):\n",
    "        return \"도움 요청\"\n",
    "    elif token_set.intersection(math_queries):\n",
    "        return \"수학 질문\"\n",
    "    else:\n",
    "        return \"알 수 없음\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8f03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (0.6.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09a4a8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkonlpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "!pip install konlpy tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b300cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 02:37:47.244763: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-18 02:37:47.387577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-18 02:37:51.056898: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae18185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/flask_app/venv310/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad01517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"안녕하세요\", \"오늘 날씨 어때요?\", \"문 좀 열어줘\", \"몇 시야?\", \"안녕 반가워요\",\n",
    "    \"음악 틀어줘\", \"내일 일정 알려줘\", \"밖에 비 와?\", \"도움 필요해\", \"영화 추천해줘\"\n",
    "]\n",
    "labels = [\n",
    "    \"인사\", \"질문\", \"명령\", \"질문\", \"인사\",\n",
    "    \"명령\", \"질문\", \"질문\", \"명령\", \"명령\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b9eb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕하세요'],\n",
       " ['오늘', '날씨', '어때요', '?'],\n",
       " ['문', '좀', '열어줘'],\n",
       " ['몇', '시야', '?'],\n",
       " ['안녕', '반가워요'],\n",
       " ['음악', '틀어줘'],\n",
       " ['내일', '일정', '알려줘'],\n",
       " ['밖에', '비', '와', '?'],\n",
       " ['도움', '필요해'],\n",
       " ['영화', '추천', '해줘']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "tokenized = [okt.morphs(s) for s in sentences]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab3089bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'안녕하세요': 1,\n",
       " '오늘': 2,\n",
       " '날씨': 3,\n",
       " '어때요': 4,\n",
       " '?': 5,\n",
       " '문': 6,\n",
       " '좀': 7,\n",
       " '열어줘': 8,\n",
       " '몇': 9,\n",
       " '시야': 10,\n",
       " '안녕': 11,\n",
       " '반가워요': 12,\n",
       " '음악': 13,\n",
       " '틀어줘': 14,\n",
       " '내일': 15,\n",
       " '일정': 16,\n",
       " '알려줘': 17,\n",
       " '밖에': 18,\n",
       " '비': 19,\n",
       " '와': 20,\n",
       " '도움': 21,\n",
       " '필요해': 22,\n",
       " '영화': 23,\n",
       " '추천': 24,\n",
       " '해줘': 25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for sent in tokenized:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "vocab            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59d5b81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [2, 3, 4, 5],\n",
       " [6, 7, 8],\n",
       " [9, 10, 5],\n",
       " [11, 12],\n",
       " [13, 14],\n",
       " [15, 16, 17],\n",
       " [18, 19, 20, 5],\n",
       " [21, 22],\n",
       " [23, 24, 25]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = [[vocab[word] for word in sent] for sent in tokenized]\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "382cef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0],\n",
       "       [ 2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  0],\n",
       "       [ 9, 10,  5,  0],\n",
       "       [11, 12,  0,  0],\n",
       "       [13, 14,  0,  0],\n",
       "       [15, 16, 17,  0],\n",
       "       [18, 19, 20,  5],\n",
       "       [21, 22,  0,  0],\n",
       "       [23, 24, 25,  0]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(seq) for seq in encoded_sentences)\n",
    "padded_sentences = pad_sequences(encoded_sentences, maxlen=max_len, padding='post')\n",
    "padded_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "360360f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe830a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sentences, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "869d3820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = Sequential([\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     Embedding(\u001b[38;5;28mlen\u001b[39m(vocab) + \u001b[32m1\u001b[39m, \u001b[43membedding_dim\u001b[49m), Conv1D(\u001b[32m64\u001b[39m, \u001b[32m3\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m), GlobalMaxPooling1D(), Dense(\u001b[32m32\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m), Dense(\u001b[38;5;28mlen\u001b[39m(le.classes_), activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_dim' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(len(vocab) + 1, embedding_dim), Conv1D(64, 3, activation='relu'), GlobalMaxPooling1D(), Dense(32, activation='relu'), Dense(len(le.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c22362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 50\n",
    "model = Sequential([\n",
    "   Embedding(len(vocab) + 1, embedding_dim),\n",
    "   Conv1D(64, 3, activation='relu'),\n",
    "   GlobalMaxPooling1D(),\n",
    "   Dense(32, activation='relu'),\n",
    "   Dense(len(le.classes_), activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50dfd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 04:21:17.961212: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc7ec4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 1s - 1s/step - accuracy: 0.3750 - loss: 1.0923\n",
      "Epoch 2/10\n",
      "1/1 - 0s - 61ms/step - accuracy: 0.3750 - loss: 1.0787\n",
      "Epoch 3/10\n",
      "1/1 - 0s - 434ms/step - accuracy: 0.5000 - loss: 1.0658\n",
      "Epoch 4/10\n",
      "1/1 - 0s - 38ms/step - accuracy: 0.6250 - loss: 1.0538\n",
      "Epoch 5/10\n",
      "1/1 - 0s - 39ms/step - accuracy: 0.7500 - loss: 1.0425\n",
      "Epoch 6/10\n",
      "1/1 - 0s - 80ms/step - accuracy: 0.7500 - loss: 1.0316\n",
      "Epoch 7/10\n",
      "1/1 - 0s - 53ms/step - accuracy: 0.7500 - loss: 1.0201\n",
      "Epoch 8/10\n",
      "1/1 - 0s - 42ms/step - accuracy: 0.8750 - loss: 1.0083\n",
      "Epoch 9/10\n",
      "1/1 - 0s - 50ms/step - accuracy: 0.8750 - loss: 0.9961\n",
      "Epoch 10/10\n",
      "1/1 - 0s - 58ms/step - accuracy: 0.8750 - loss: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e6f2a6dd430>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082bc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239abf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification_cnn_model.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"text_classification_cnn_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f52beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "문장: 안녕하세요!\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 문 좀 열어줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 현재 시각이 궁금해.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 창문을 닫아.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 오늘 날씨 어때?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 도와줄 수 있어?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 잘 지내?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 동영상 틀어줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 2 더하기 2는 뭐야?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 영화 추천해줘.\n",
      "→ 예측 의도: 명령\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "   \"안녕하세요!\", \"문 좀 열어줘.\", \"현재 시각이 궁금해.\", \"창문을 닫아.\",\n",
    "   \"오늘 날씨 어때?\", \"도와줄 수 있어?\", \"잘 지내?\", \"동영상 틀어줘.\",\n",
    "   \"2 더하기 2는 뭐야?\", \"영화 추천해줘.\"\n",
    "]\n",
    "\n",
    "\n",
    "test_tokenized = [okt.morphs(s) for s in test_sentences]\n",
    "test_encoded = [[vocab.get(word, 0) for word in sent] for sent in test_tokenized]  # 없는 단어는 0\n",
    "test_padded = pad_sequences(test_encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "\n",
    "pred_probs = model.predict(test_padded)\n",
    "pred_labels = le.inverse_transform(np.argmax(pred_probs, axis=1))\n",
    "\n",
    "\n",
    "for sent, label in zip(test_sentences, pred_labels):\n",
    "   print(f\"문장: {sent}\\n→ 예측 의도: {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2be86d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = joblib.load('text_classification_cnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a5e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "문장: 안녕, 오늘 기분 어때?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 내일 비 올까?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 음악 재생해줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 시간 알려줘.\n",
      "→ 예측 의도: 명령\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\n",
    "   \"안녕, 오늘 기분 어때?\",\n",
    "   \"내일 비 올까?\",\n",
    "   \"음악 재생해줘.\",\n",
    "   \"시간 알려줘.\"\n",
    "]\n",
    "new_tokenized = [okt.morphs(s) for s in new_sentences]\n",
    "new_encoded = [[vocab.get(word, 0) for word in sent] for sent in new_tokenized]\n",
    "new_padded = pad_sequences(new_encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "\n",
    "new_pred_labels = le.inverse_transform(np.argmax(new_pred_probs, axis=1))\n",
    "\n",
    "for sent, label in zip(new_sentences, new_pred_labels):\n",
    "   print(f\"문장: {sent}\\n→ 예측 의도: {label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
